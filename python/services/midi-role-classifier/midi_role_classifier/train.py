"""
Training script for the MIDI voice role classifier.

Bootstraps training data by running the Rust heuristic classifier on a MIDI
corpus, then trains a GradientBoostingClassifier on the extracted features.

Usage:
    python -m midi_role_classifier.train [--data-dir PATH] [--output PATH]

Data format:
    Expects JSON files in data-dir, each containing a list of objects with
    'features' (VoiceFeatures dict) and 'role' (string label) fields.
    These can be generated by calling midi_classify_voices with use_ml=false
    and saving the features_json + classifications.
"""

from __future__ import annotations

import argparse
import json
import logging
import os
from pathlib import Path

import numpy as np
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report

from .model import FEATURE_NAMES, VOICE_ROLES, features_to_array, MODEL_DIR, MODEL_PATH

log = logging.getLogger(__name__)


def load_training_data(data_dir: str) -> tuple[np.ndarray, np.ndarray]:
    """Load labeled feature vectors from JSON files."""
    all_features = []
    all_labels = []

    data_path = Path(data_dir)
    if not data_path.exists():
        raise FileNotFoundError(f"Training data directory not found: {data_dir}")

    for json_file in sorted(data_path.glob("*.json")):
        try:
            with open(json_file) as f:
                samples = json.load(f)

            for sample in samples:
                features = sample.get("features")
                role = sample.get("role")
                if features and role and role in VOICE_ROLES:
                    all_features.append(features)
                    all_labels.append(role)

        except (json.JSONDecodeError, KeyError) as e:
            log.warning(f"Skipping {json_file}: {e}")
            continue

    if not all_features:
        raise ValueError(f"No valid training samples found in {data_dir}")

    X = features_to_array(all_features)
    y = np.array(all_labels)

    log.info(f"Loaded {len(X)} samples from {data_dir}")

    # Log class distribution
    unique, counts = np.unique(y, return_counts=True)
    for label, count in zip(unique, counts):
        log.info(f"  {label}: {count} samples")

    return X, y


def train(
    data_dir: str,
    output_path: str | None = None,
    n_estimators: int = 200,
    max_depth: int = 5,
) -> None:
    """Train the classifier and save the model."""
    import joblib

    output_path = output_path or MODEL_PATH

    X, y = load_training_data(data_dir)

    log.info(f"Training GradientBoostingClassifier (n={n_estimators}, depth={max_depth})...")
    model = GradientBoostingClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        learning_rate=0.1,
        random_state=42,
    )

    # Cross-validation
    if len(X) >= 10:
        cv_scores = cross_val_score(model, X, y, cv=min(5, len(np.unique(y))), scoring="accuracy")
        log.info(f"Cross-validation accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})")

    # Train on full data
    model.fit(X, y)

    # Classification report
    y_pred = model.predict(X)
    report = classification_report(y, y_pred, zero_division=0)
    log.info(f"Training set classification report:\n{report}")

    # Feature importance
    importances = model.feature_importances_
    sorted_idx = np.argsort(-importances)
    log.info("Top 10 feature importances:")
    for i in sorted_idx[:10]:
        log.info(f"  {FEATURE_NAMES[i]}: {importances[i]:.4f}")

    # Save model
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    joblib.dump(model, output_path)
    log.info(f"Model saved to {output_path}")


def main():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(name)s: %(message)s",
    )

    parser = argparse.ArgumentParser(description="Train MIDI voice role classifier")
    parser.add_argument(
        "--data-dir",
        default=os.path.join(MODEL_DIR, "training-data"),
        help="Directory containing labeled training data JSON files",
    )
    parser.add_argument(
        "--output",
        default=MODEL_PATH,
        help="Output path for trained model",
    )
    parser.add_argument("--n-estimators", type=int, default=200)
    parser.add_argument("--max-depth", type=int, default=5)

    args = parser.parse_args()
    train(args.data_dir, args.output, args.n_estimators, args.max_depth)


if __name__ == "__main__":
    main()
