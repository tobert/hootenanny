# HalfRemembered MCP: Implementation Vision and Roadmap

This document outlines a strategic vision for implementing the HalfRemembered MCP, synthesizing the core ideas from the three foundational research documents:
1.  **Gemini Research:** The high-level architectural philosophy.
2.  **Claude Research:** The detailed, bottom-up domain model of musical objects.
3.  **System Dynamics Research:** The low-level patterns for persistence, collaboration, and real-time safety.

## The Guiding Vision: The "Event Duality" Engine

Our core architecture will be built around the **"Event Duality"** paradigm. The central nervous system of our application will be the **Universal Timeline**, a novel data structure that holds two co-equal types of events:
1.  **`ConcreteEvents`**: High-resolution performance data (Notes, MIDI 2.0 expression), representing *what happened*.
2.  **`AbstractEvents`**: Generative instructions (Prompts, Constraints), representing *what to do*.

**Agentic Streams** (our replacement for "tracks") will be the actors that process these events. A `PerformanceStream` will capture human input as `ConcreteEvents`. An `AgenticStream` will listen for `AbstractEvents` and generate new `ConcreteEvents`, guided by the shared **MusicalContext**.

This entire system will be built in Rust, targeting the **CLAP plugin standard** to ensure the end-to-end integrity of our high-resolution MIDI 2.0 data model.

## The Implementation Roadmap

I propose a four-phase plan to build this system from the ground up. This approach allows us to tackle complexity incrementally, ensuring that each layer is built on a solid, tested foundation.

### Phase 1: The Core Data Model (The Vocabulary)

**Goal:** Translate the conceptual models into immutable, serializable Rust structs and enums. This is the foundation.

1.  **Define the Event Duality:** Create the primary `Event` enum with `Concrete` and `Abstract` variants.
2.  **Implement Concrete Payloads:** Build the Rust structs for `Note`, `ExpressionStream`, and other performance data, strictly following the MIDI 2.0 data model.
3.  **Implement Abstract Payloads:** Build the structs for `PromptEvent`, `ConstraintEvent`, and `OrchestrationEvent`.
4.  **Build the World Model:** Implement the `MusicalContext` entity, representing the time-stamped maps for key, tempo, and chords.
5.  **Ensure Serialization:** Add `serde` support to all data structures to prepare for persistence.

**Outcome:** A library of pure data structures that represents our entire musical domain. No logic, just the types.

### Phase 2: The Non-Real-Time Engine (The Brain)

**Goal:** Build the main application logic that orchestrates events outside of the strict audio thread.

1.  **The Universal Timeline:** Implement the timeline data structure itself. It must be able to hold the tree of event sequences and manage branching possibilities.
2.  **Persistence Layer:** Using `sled`, implement the **Hybrid Storage Model** from the dynamics research. We will store the core project structure, track chunks, and shared resources as separate entities.
3.  **The First Agentic Loop:** Create a basic `AgenticStream` processor. Its first task will be simple: consume a `ConstraintEvent` from the timeline, query the `MusicalContext`, and generate `ConcreteEvent` notes that adhere to the specified key/scale.

**Outcome:** A command-line-drivable "engine" that can load a project, process a simple generative instruction, and save the result. The core agentic loop is proven.

### Phase 3: The Real-Time Engine (The Heartbeat)

**Goal:** Create the audio-thread-safe playback engine. This is the most technically demanding phase.

1.  **CLAP Plugin Scaffolding:** Set up the basic structure for a CLAP audio plugin.
2.  **The "Flattened Timeline":** Implement the critical optimization from the dynamics research. The "Brain" (Phase 2) will be responsible for analyzing the complex Universal Timeline and producing a simple, linear, pre-computed schedule of events for the audio thread.
3.  **Lock-Free Communication:** Use a **Ring Buffer** to pass `ScheduledEvent` data from the Brain to the Heartbeat. This is essential for real-time safety.
4.  **Simple Synthesizer:** Build a minimal internal synth voice that can take a `ScheduledEvent` and produce a sine wave.

**Outcome:** A working CLAP plugin that can play back a sequence of notes generated by the non-real-time engine, proving the entire data flow from abstract instruction to audio output is viable and performant.

### Phase 4: The Collaborative Layer (The Soul)

**Goal:** Build the advanced human-AI interaction features on top of the working core.

1.  **The Suggestion Layer:** Implement the "Parallel Reality" model from the dynamics research. An `AgenticStream` will now publish its output as a `Suggestion` rather than directly to the main timeline.
2.  **Human-in-the-Loop:** Create a simple interface (TUI or CLI) that allows a human to see incoming suggestions, preview them (triggering playback), and then accept (merge) or reject them.
3.  **Advanced Agents:** Evolve the `AgenticStream` to handle more complex `PromptEvents` (natural language) and `OrchestrationEvents` (multi-step tasks).

**Outcome:** A truly interactive system that fulfills the "HalfRemembered" vision. A human can provide a fragment of an idea, and the AI can offer completions that the human can then curate.
